---
title: Lecture_09_Transformation_of_models
separator: <!--s-->
verticalSeparator: <!--v-->
theme: simple
highlightTheme: github
css: assets/custom.css
revealOptions:
    transition: 'slide'
    transitionSpeed: fast
    center: false
    slideNumber: "c/t"
    width: 1000
    pdfSeparateFragments: false
---

<div style="display: flex; justify-content: center; align-items: center; height: 700px;">
  <div style="text-align: center; padding: 40px; background-color: white; border: 2px solid rgb(0, 63, 163); border-radius: 20px; box-shadow: 0 0 20px rgba(0,0,0,0.1);">
    <h1 style="font-size: 48px; font-weight: bold; margin-bottom: 20px; color: #333;">SI100+ 2024 Lecture 9</h1>
    <p style="font-size: 24px; color: #666;">力大砖飞的时代——模型的转变</p>
    <p style="font-size: 16px; color: #999; margin-top: 20px;">SI100+ 2024 Staff | 2024-09-11</p>
  </div>
</div>

<!--s-->

## Intro：GPT模型参数爆炸式增长

从2018年起，OpenAI就开始发布**生成式预训练语言模型GPT**（Generative Pre-trained Transformer），可用于生成文章、代码、机器翻译、问答等各类内容。每一代GPT模型的参数量都爆炸式增长，堪称“越大越好”。2019年2月发布的GPT-2参数量为15亿，而2020年5月的GPT-3，参数量达到了1750亿。从下面的参数量可以看出，这是个多么恐怖的模型。

![alt text](image.png)

- 最新的GPT-4在120层中总共包含了1.8万亿参数，规模是GPT-3的10倍以上
- 这里有 [一个神奇的网站](https://bbycroft.net/llm) ，可视化地告诉了我们这个庞大的差距
- 你可能有疑问

<!--v-->

## 你可能有疑问

- 为什么LLM发展的趋势是不断提高参数量？
- 或者说，增加参数量是如何让LLM的performance变的更好的？
- 通过这节课的学习，你或许能找到答案

<!--s-->

## Re：从零开始的ML学习

- 上节课我们简单介绍了一些ML的基础知识，包括它的分类，任务以及简单原理。
- 还记得高中的线性回归如何求解吗？最小二乘法求出经验回归方程: $\hat{y} = \hat{b} x + \hat{a}$
- 为什么叫做最小二乘法？二乘：（残差的）平方；最小：使……最小
  - 即 $L = \frac{1}{2n} \sum_{i=1}^{n} (\hat{y}_i-f(x_i))^2$，求 $\min(L)$
    - P.S. 其实这本质上也是一种 **损失函数**（用来量化表示模型预测的偏差）
- 如何找到最小？这对于学过了三年高中数学的你可能非常简单：只需要 **求导** 就好了
- 最小二乘法的求导一定能有解（不然高中提供的计算公式怎么来的呢）

<img  src="image-2.png" width="400" style="display: block; margin: 0 auto;"/>

<!--v-->

<!-- 这里我不能确定是不是所有的线性回归都有解析解 -->

## 但是求导一定能找到准确的最小吗？

- 当然不是
- 并不是所有的 ML 问题都有解析解，甚至大部分回归问题都没有准确的解析解
- 等等，什么是解析解？

> 例：（2017 年全国 II 卷）21. $f(x) = x^2 -x -x \ln x$.  
> 证明：$f(x)$ 存在唯一的极大值 $x_0$，且 $e^{-2}<f(x_0)<2^{-2}$

- 经常做导数题的大家想必都知道，这是一道经典的隐零点问题
- 求个导，令 $f'(x) = 2x - 2 - \ln x$，求解 $f'(x_0) = 0$ ，这里的 $x_0$ 显然是无法直接写成一个 **由有限次常见运算给出的表达式** 的，也就是 **无解析解** (也被称为无公式解)
- 怎么办呢？

<!--v-->

## 逼近数值解

- 当解析解不存在时，比如五次以及更高次的代数方程，则该方程只能用数值分析的方法求解近似值。大多数偏微分方程，尤其是非线性偏微分方程，都只有数值解。
- 求数值解的目的不在求出正确的答案，而是在其误差在一合理范围的条件下找到近似解。
- 有什么方法可以越来越逼近呢？
  - 中学常见的有二分法、牛顿迭代法
  - 他们有什么共同点？

<!--v-->

## 梯度下降法

- 事实上，我们从始至终的目标没有变过：**使损失函数（Loss function）达到最小值**，这里我们介绍一个新的方法：梯度下降法

- 梯度下降法是一种**优化方法**，是一种通过求目标函数的导数来寻找目标函数最小值的方法，**最终求得的是逼近数值解**

<!--v-->

## 梯度下降法 (cont'd)

- **梯度就是导数**
- “沿着梯度方向”就是函数变化趋势最快的方向
- 假设一个这样的场景，一个人被困在山上，需要从山上下来(找到山的最低点，也就是山谷)，但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。
- 如果采用梯度下降算法，就是**以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的方向走**，然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷

<img src="image-5.png" width="700" style="display: block; margin: 0 auto;"/>

<!--v-->

## 梯度下降法
 
<img  src="image-5.png" width="500" style="display: block; margin: 0 auto;"/>

按照梯度下降法：

1. 明确自己现在所处的位置
2. 找到相对于该位置而言下降最快的方向
3. 沿着第二步找到的方向走一小步，到达一个新的位置，此时的位置肯定比原来低
4. 回到第一步，如此循环往复，直到终止于最低点

<!-- ## 数学上的解释

<p align = "center">    
<img  src="image-6.png" width="500" />
</p>

定义如上图的公式，J是关于Θ的一个函数，我们在山林里当前所处的位置为$Θ^0$点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度$\bigtriangledown J(\theta )$的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了$Θ^1$这个点。

<p align = "center">    
<img  src="image-5.png" width="500" />
</p>


## 学习率 $\alpha$

<p align = "center">    
<img  src="image-7.png" width="400" />
</p>

* α在梯度下降算法中被称作为**学习率**，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。

* 所以**α的选择在梯度下降法中往往是很重要的**！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！

<p align = "center">    
<img  src="image-8.png" width="400" />
</p>

-->

<!--s-->

## 感知机的拓展：从二维到 $n$ 维

<!--v-->

## 回顾：感知机

- 在之前的课程中我们介绍了一个非常简单的感知机（perceptron）模型
- 在之前的例子中，我们构造了一个“模型”来对一系列带有标记 $y$ 的点 $(x_1, x_2)$ 的集合 $\left\\{ (x_1^{(i)}, x_2^{(i)},y^{(i)}) \right\\}$ 进行二分类
- 于是我们就能使用这个模型来预测别的点的类别了
- 这个模型能接受两个输入（$x_1$ 和 $x_2$），并且提供一个输出 $y$，那么它就可以长这样：

<img src="image-14.png" width="250" style="display: block; margin: 0 auto;"/>

- P.S. 看到箭头上的 $w_i$，你可能会对 **权重向量** $\boldsymbol{w}$ 有了更深的理解(权重值越大，该输入越重要)

<!--v-->

## 感知机有几层？

<img src="image-14.png" width="250" style="display: block; margin: 0 auto;"/>

- 一看就是两层：输入层，输出层
- 输入层用来接受输入信号，并且传递给输出层
- 输出层干了什么呢？
  - 我们知道 $\boldsymbol{w} \cdot \boldsymbol{x}$ 不一定是一个整数，但是不会有 “$0.114514$ 好的瓜”
  - 因此它就像一个开关，将输入的信号 **两极化**，要么是 $1$，要么是 $0$
  - 输入信号……传递信号……有没有很熟悉？
  - 兴奋在神经元上的传导！

<!--v-->

## 这里也有神经元！

<div style="column-count:2">

- 生物神经网络中，每个神经元与其他神经元相连，当它 “兴奋” 时，就会向相连的神经元释放神经递质，从而改变这些神经元内的电位。
- 如果某神经元的电位超过了一个“阈值”,那么它就会被激活，即 “兴奋”起来，向其他神经元发送化学物质.
- 在刚刚的图中，每一个 `○→` 都是一个神经元 (neuron)，接收别的神经元的信号，这些输入信号通过带权重的连接 (connection) 进行传递，总输入值超过该神经元的阈值 (threshold) 就会 “兴奋”
- 这就是 “M-P 神经元模型” (1943)

<img src="image-15.png" width="400" style="display: block; margin: 0 auto;"/>

</div>

<!--v-->

## 还可不可以更劲爆点？


- 这个模型只接受二维 $(x_1,x_2)$ 的输入，思考一下，如果输入是三个，四个，乃至 $n$ 个（三维，四维到 $n$ 维）呢？

<img src="image-16.png" width="500" style="display: block; margin: 0 auto;"/>

<div style="column-count:2">

- 这里的 $f$ 是一个 **激活函数**，顾名思义，**理想中** 用来将传入的总输入值与阈值的差映射到 $0$, $1$ ，用来判断是否激活，如右图左的 **阶跃函数**。
- 但其实它并不常见，因为它的函数不连续也不光滑，更常见的是如右图右的 Sigmoid 函数，我们不多做介绍

<img src="image-17.png" width="800" style="display: block; margin: 0 auto;"/>

</div>

<!--s-->

## 深度神经网络的诞生：多层感知机（MLP）

<!--v-->

## 可以更广，也可以更深！

- 刚刚我们在输入层面拓展了感知机模型，使之可以接受若干个输入
- 那我们是否也可以给感知机堆叠层数使之更加复杂呢？
- 在生物学上非常容易理解：我们让多个神经元头尾相接！就可以构成一个有更多功能的神经网络！
- 为此，我们引入**多层感知机**（Multi-Layer Perception）

<!--v-->

## 多层感知机

- 还记得我们的最开始的感知机不能解决的问题吗，这其实是计算机界大名鼎鼎的 **异或 (XOR)**

<img src="image-18.png" width="300" style="display: block; margin: 0 auto;"/>

- 这不是线性可分的问题！也就是我们无法画一条线分开所有的正点和负点！
- 但是，再加一层神经元就可以 😉

<img src="image-19.png" width="500" style="display: block; margin: 0 auto;"/>

<!--v-->

## 太神奇了！

- 我们只添加了一层就解决了这个问题

<img src="image-19.png" width="500" style="display: block; margin: 0 auto;"/>

- 添加的这一层叫做 **隐层 或 隐含层(hidden layer)** 
  - 既不是输入也不是输出，我们没有办法直接观测到，就像被藏起来了
- 隐含层和输出层神经元都是拥有 **激活函数** 的功能神经元
  - 输入层神经元仅是接受输入，不进行函数处理，因此在计算层数的时候通常忽略输入层

<!--v-->

## 层级结构

- 更一般的，常见的神经网络是下图所示的层级结构
- 每层神经元与下一层神经元 **全互连**，神经元之间 **不存在同层连接**，也 **不存在跨层连接**
- 输入层神经元接收外界输入，隐层与输出层神经元对信号进行
加工，最终结果由输出层神经元输出

<p align = "center">    
<img  src="image-11.png" width="350" />
</p>

<!--v-->

## 神经网络学习的本质

> 神经网络的学习过程，就是根据训练数据来调整神经元之间的
 **“连接权重”(connection weight)** 以及每个功能神经元的 **阈值** ；换言之，神经网络 “学” 到的东西，蕴涵在 **连接权与阈值中**


- 这里有一个[小的在线演示](https://playground.tensorflow.org/#activation=tanh&batchSize=29&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2,1&seed=0.54302&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)，我们可以感性且直观地理解一下训练模型的过程

<!-- - 首先它与输入层是全连接的，类似于简单的感知机结构，假设输入层用向量 $X$ 表示，则隐藏层的输出就是 $f (W_1X+b_1)$，$W_1$是权重（也叫连接系数），$b_1$ 是偏置， -->
<!-- 函数 $f$ 可以是 **激活函数** (常用的sigmoid函数，tanh函数，RELU函数等) -->
- 激活函数？解决了感知机只能进行线性分类的问题
    - 不使用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。
    - 使用激活函数，能够给神经元引入非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以利用到更多的非线性模型中。

<!--v-->

## MLP 的简单训练过程

> 训练的过程就是一个优化参数的过程

- 刚刚我们看到的都是数据单向地从输入不断流向下一层，最终流向输出，没有任何循环。这样的网络叫做 **前馈神经网络 (Feedforward Neural Network, FNN)**，它的基础训练方式是 **前向传播 (Forward Propagation, FP)**
  - 前向传播：输入层数据开始从前向后，数据逐步传递至输出层
- 但是现代的 FNN 还会有另外一个步骤：**反向传播 (Backpropagation, BP)**
  <!-- - 反向传播：损失函数开始从后向前，梯度逐步传递至第一层。反向传播用于权重更新，使网络输出更接近标签

  - 在反向传播中，会使用**梯度下降法**更新参数以减小损失函数的值 -->

<!--v-->

## 反向传播的步骤

- 反向传播算法（BP 算法）主要由两个阶段组成：激励传播与权重更新。
- 激励传播中，**每次迭代** 将会
  1. （前向传播阶段）将训练输入送入网络以获得预测结果
  2. （反向传播阶段）对预测结果同训练目标求差 (损失函数)
- 权重更新时，会对每个突触 (连接) 上的权重
  1. （获取梯度）将输入激励和响应误差相乘，从而获得权重的梯度
  2. （更新权重）将这个梯度乘上一个比例 (训练因子) 并取反后加到权重上

- 有点抽象？再次类比一下我们下山的问题
  - 前向传播 = 往当前的方向走一步
  - 反向传播 = 看看海拔离目标海拔的差距
  - 获取梯度 = 看看往哪里走差距下降的最快
  - 更新权重 = 调整方向

<!--s-->

# 走向模块化的神经网络

<!--v-->

## 层和块

- 我们刚刚已经做出了一个理想中的神经网络小部件了！
- 它是一个 XOR 小部件，可以：接受 2 个输入，产生 1 个输出！
- 那我们可不可以封装一下，让它变成一个“黑盒子”，我们只需要拿来用就行，而不用再关心里面的实现了？
- 当然可以！

<img src="https://zh.d2l.ai/_images/blocks.svg" width="700" style="display: block; margin: 0 auto;"/>

<!-- * 之前首次介绍神经网络时，我们关注的是具有单一输出的线性模型（感知机）。 在这里，整个模型只有一个输出。  
注意，单个神经网络 （1）接受一些输入； （2）生成相应的输出； （3）具有一组相关参数（parameters），更新这些参数可以优化某目标函数。

* 对于多层感知机而言，我们可以认为整个模型及其组成层都是这种架构 -->
<!-- 
## 层和块

* 对于多层感知机而言，我们可以认为整个模型及其组成层都是这种架构

* 整个模型接受原始输入（特征），生成输出（预测）， 并包含一些参数（所有组成层的参数集合）。 同样，每个单独的层接收输入（由前一层提供）， 生成输出（到下一层的输入），并且具有一组可调参数， 这些参数根据从下一层反向传播的信号进行更新。

<p align = "center">    
<img  src="https://zh.d2l.ai/_images/blocks.svg" width="700" />
</p> -->

<!--v-->

## 层和块

- 为了更好的实现更加复杂的网络，我们引入了神经网络块的概念。 

- **块 (block)** 可以描述单个层、或多个层组成的组件，甚至整个模型本身！使用块进行抽象的一个好处是可以将一些块组合成更大的组件，使我们可以通过更简洁的代码实现复杂的神经网络。
- 块的概念十分宽泛，MLP 的任意一层可以看成一个**块**，整个MLP 也能看做一个**块**

<img src="https://zh.d2l.ai/_images/blocks.svg" width="700" style="display: block; margin: 0 auto;"/>

<!--v-->

## 模块的力量

- `PyTorch` 的 `TorchVision` 项目，包含了一些表现优异的、关于计算机视觉的神经网络架构，我们介绍一下其中两个代表性的
- AlexNext: 它是在图像识别方面早期具有突破性的网络之一。在 2012 年的 ILSVRC 中以较大的优势胜出，前 5 名的测试错误率（也就是说，正确的标签必须在前 5 名中）为 15.4%。相比之下，没有深度网络的次好作品仅占 26.2%。这是计算机视觉历史上的一个关键时刻：此刻，社区开始意识到深度学习在视觉任务中的潜力。随之而来的是不断的改进，更现代的架构和训练方法使得前 5 名的错误率低至 3%。
- ResNet(残差网络): 它在 2015 年的 ILSVRC 中获胜。目前ResNet架构仍然是许多视觉任务的首选架构。
  - ResNet101 有 101 层，这些层是由层组（groups of layers）的重复模式组成，我们可以把每一层当作一个模块
- 在其他的领域，如自然语言处理和语音， 层组以各种重复模式排列的类似架构现在也是普遍存在。这些层组都可以看成一个一个模块。
- 我们做了一个[小的演示](https://www.kaggle.com/code/zambar/si100-pretrained-models)，让大家体验一下预训练好的一个图像识别的 ResNet101 模型

<!--s-->

# 深度学习的发展：研究方向的细分和新模型的提出

* 随着深度学习的发展和研究方向的细化，CV（计算机视觉，computer vision）和NLP（自然语言处理，Natuarl Language Processing）成为深度学习发展最迅速、最具前景的两大分支

---

## CV与CNN网络架构

* CNN，即卷积神经网络（Convolutional Neural Networks），是一种专门用于处理具有类似网格结构数据（如图像）的深度学习模型。

* 它利用卷积核（也称为滤波器或kernel）在输入图像上进行滑动窗口操作，来获取“感受野”范围内数据之间的关系特征。一张图片里，相邻的像素显然是有更强的相关性，相比于MLP，CNN突出了这种相邻的关系特征，因而更加准确的获取了图片内的有用信息。

<p align = "center">    
<img  src="kernel.gif" width="400" />
</p>

---

## NLP与RNN网络架构

* RNN，即循环神经网络（Recurrent Neural Networks），是一种用于处理序列数据（如语言）的深度学习模型。

* 普通的神经网络缺乏记忆能力，因此在处理序列数据时表现不佳。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。

* RNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中。

<p align = "center">    
<img  src="image-13.png" width="400" />
</p>

---

## 现在的神经网络发展：从功能化的堆叠到纯粹的暴力堆层

* 我们可以把一个神经网络视作一个块，将几个已知的拥有不同功能的神经网络堆叠在一起，就可以实现不同的下游任务

* 现在的网络已经回归到了纯粹的堆叠上，不断重复相同的块，在猛堆参数量的条件下，依靠暴力依旧能取得很好的效果（如GPT，LLAMA等架构）

<p align = "center">    
<img  src="LLAMA.png" height="400" />
</p>